# JAX Tutorial: Getting Started with Google's High-Performance Numerical Computing Library

# 1. Installation
# CPU version
# pip install jax

# GPU version (CUDA 11)
# pip install jax[cuda11] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html

# 2. Basic Operations
import jax
import jax.numpy as jnp

# Check JAX version and available devices
print("JAX version:", jax.__version__)
print("Available devices:", jax.devices())

# Create arrays
x = jnp.array([1, 2, 3, 4])
y = jnp.ones((3, 3))

# Basic operations
z = jnp.dot(x, x)
print("Dot product:", z)  # Output: 30 (1*1 + 2*2 + 3*3 + 4*4)

# 3. Core Features: Automatic Differentiation
from jax import grad, jit, vmap

# Define a function
def f(x):
    return jnp.sum(x**2)

# Compute gradient
df_dx = grad(f)
print("Gradient:", df_dx(jnp.array([1.0, 2.0, 3.0])))  # Output: [2. 4. 6.]

# Use JIT compilation for acceleration
fast_f = jit(f)
fast_df_dx = jit(df_dx)

# 4. Vectorization and Parallel Processing
# Vectorized operations (vmap)
def scalar_fn(x):
    return x**2

# Map scalar function to vector
vectorized_fn = vmap(scalar_fn)
x = jnp.array([1.0, 2.0, 3.0])
print("Vectorized result:", vectorized_fn(x))  # Output: [1. 4. 9.]

# Batch matrix multiplication
batch_size = 10
mat_dims = 3
batch_a = jnp.ones((batch_size, mat_dims, mat_dims))
batch_b = jnp.ones((batch_size, mat_dims, mat_dims))

# Use vmap for batched matrix multiplication
batched_matmul = vmap(jnp.matmul)
result = batched_matmul(batch_a, batch_b)
print("Batch matmul shape:", result.shape)

# 5. Random Number Generation
# Create a PRNG key
key = jax.random.PRNGKey(42)

# Generate random numbers
key, subkey = jax.random.split(key)
random_numbers = jax.random.normal(subkey, shape=(5,))
print("Random numbers:", random_numbers)

# Generate multiple random arrays
key, *subkeys = jax.random.split(key, num=4)
arrays = [jax.random.normal(k, (3, 3)) for k in subkeys]
print("Number of random arrays:", len(arrays))

# 6. Neural Network Example
# Initialize parameters
def init_network_params(sizes, key):
    """Initialize the weights and biases for a fully-connected neural network."""
    keys = random.split(key, len(sizes))
    return [
        {'w': random.normal(k, (m, n)) * 0.01, 
         'b': jnp.zeros(n)}
        for k, m, n in zip(keys, sizes[:-1], sizes[1:])
    ]

# Forward pass
def forward(params, x):
    """Forward pass through the network."""
    activations = x
    for layer in params:
        x = jnp.dot(activations, layer['w']) + layer['b']
        activations = jnp.tanh(x)
    return activations

# Loss function
def loss(params, x, y):
    """Mean squared error loss."""
    predictions = forward(params, x)
    return jnp.mean((predictions - y) ** 2)

# Update function with automatic differentiation
@jit
def update(params, x, y, learning_rate=0.01):
    """Update parameters using gradient descent."""
    grads = grad(loss)(params, x, y)
    return [
        {'w': layer['w'] - learning_rate * g['w'], 
         'b': layer['b'] - learning_rate * g['b']}
        for layer, g in zip(params, grads)
    ]

# Example usage of neural network
def train_example():
    # Generate some fake data
    key = random.PRNGKey(0)
    key, subkey = random.split(key)
    x = random.normal(subkey, (100, 10))
    y = random.normal(subkey, (100, 1))
    
    # Initialize network
    layer_sizes = [10, 20, 15, 1]
    key, subkey = random.split(key)
    params = init_network_params(layer_sizes, subkey)
    
    # Train for a few steps
    for i in range(100):
        params = update(params, x, y)
        if i % 10 == 0:
            current_loss = loss(params, x, y)
            print(f"Step {i}, Loss: {current_loss}")
    
    return params

# Uncomment to run training
# trained_params = train_example()

# 7. JAX Ecosystem Example with Flax
# pip install flax

def flax_example():
    import flax.linen as nn
    
    class MLP(nn.Module):
        features: tuple = (32, 10)
        
        @nn.compact
        def __call__(self, x):
            for feat in self.features[:-1]:
                x = nn.relu(nn.Dense(feat)(x))
            x = nn.Dense(self.features[-1])(x)
            return x
    
    # Initialize and use the model
    model = MLP()
    key = jax.random.PRNGKey(0)
    dummy_input = jnp.ones((1, 28*28))
    variables = model.init(key, dummy_input)
    output = model.apply(variables, dummy_input)
    print("Flax MLP output shape:", output.shape)
    
# Uncomment to run Flax example
# flax_example()

# 8. Advanced Example: MNIST training with Optax
def optax_example():
    """
    This example shows how to use JAX with Optax optimizer for MNIST.
    Requires additional installations:
    pip install optax tensorflow tensorflow-datasets
    """
    import optax
    import tensorflow as tf
    import tensorflow_datasets as tfds
    
    # Load MNIST dataset
    def get_datasets():
        ds_builder = tfds.builder('mnist')
        ds_builder.download_and_prepare()
        train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=32))
        test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=32))
        return train_ds, test_ds
    
    # Define network
    def net(x, params):
        # Flatten the input from (28, 28) to a vector
        x = x.reshape((x.shape[0], -1))
        # Hidden layer
        x = jnp.dot(x, params['w1']) + params['b1']
        x = jnp.tanh(x)
        # Output layer
        x = jnp.dot(x, params['w2']) + params['b2']
        return x
    
    # Initialize parameters
    def init_params(key):
        w1_key, w2_key, b1_key, b2_key = random.split(key, 4)
        return {
            'w1': random.normal(w1_key, (28*28, 128)) * 0.1,
            'b1': random.normal(b1_key, (128,)) * 0.1,
            'w2': random.normal(w2_key, (128, 10)) * 0.1,
            'b2': random.normal(b2_key, (10,)) * 0.1
        }
    
    # Loss function
    def loss_fn(params, x, y):
        logits = net(x, params)
        one_hot_y = jax.nn.one_hot(y, 10)
        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot_y))
        return loss
    
    # Train step
    @jit
    def train_step(params, opt_state, x, y):
        loss, grads = jax.value_and_grad(loss_fn)(params, x, y)
        updates, opt_state = optimizer.update(grads, opt_state)
        params = optax.apply_updates(params, updates)
        return params, opt_state, loss
    
    # Setup
    key = random.PRNGKey(0)
    params = init_params(key)
    optimizer = optax.adam(1e-3)
    opt_state = optimizer.init(params)
    
    # Training loop
    def train():
        train_ds, test_ds = get_datasets()
        for epoch in range(5):
            # Train
            epoch_loss = 0
            steps = 0
            for batch in train_ds:
                x, y = batch['image'] / 255.0, batch['label']
                params, opt_state, loss = train_step(params, opt_state, x, y)
                epoch_loss += loss
                steps += 1
            
            print(f"Epoch {epoch+1}, Loss: {epoch_loss/steps}")
            
            # Evaluate
            correct = 0
            total = 0
            for batch in test_ds:
                x, y = batch['image'] / 255.0, batch['label']
                logits = net(x, params)
                preds = jnp.argmax(logits, axis=1)
                correct += jnp.sum(preds == y)
                total += y.shape[0]
            
            print(f"Accuracy: {correct/total:.4f}")
    
    # Uncomment to train
    # train()

# 9. JAX Transformations Summary
def transformations_example():
    from jax import jacfwd, jacrev, hessian
    
    # Function to differentiate
    def f(x):
        return jnp.sin(x[0]) + jnp.cos(x[1])
    
    # Points to evaluate at
    x = jnp.array([2.0, 3.0])
    
    # Different ways to compute derivatives
    print("Gradient:", grad(f)(x))
    print("Jacobian (forward mode):", jacfwd(f)(x))
    print("Jacobian (reverse mode):", jacrev(f)(x))
    print("Hessian:", hessian(f)(x))

# Uncomment to run
# transformations_example()

# 10. Device Management
def device_examples():
    # Print available devices
    print("Available devices:", jax.devices())
    
    # Explicitly place arrays on devices
    cpu_x = jax.device_put(jnp.ones((5, 5)), jax.devices("cpu")[0])
    
    # Try to place on GPU if available
    if any("gpu" in str(d).lower() for d in jax.devices()):
        gpu_device = next(d for d in jax.devices() if "gpu" in str(d).lower())
        gpu_x = jax.device_put(jnp.ones((5, 5)), gpu_device)
        print("Array placed on GPU")
    
    # Get current device of an array
    print("Array device:", cpu_x.device())

# Uncomment to run
# device_examples()

if __name__ == "__main__":
    print("Running JAX tutorial examples...")
    # Basic operations
    print("\n=== Basic Operations ===")
    print("JAX version:", jax.__version__)
    print("Available devices:", jax.devices())
    
    # Run gradient example
    print("\n=== Gradient Example ===")
    print("Gradient of sum(x^2):", df_dx(jnp.array([1.0, 2.0, 3.0])))
    
    # Run vectorization example
    print("\n=== Vectorization Example ===")
    print("Vectorized x^2:", vectorized_fn(jnp.array([1.0, 2.0, 3.0])))
    
    print("\nJAX tutorial completed.")
    
